{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NLP_Week4_Exercise_Shakespeare_Answer.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX4Kg8DUTKWO"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOwsuGQQY9OL"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras.utils as ku \n",
        "import numpy as np "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRnDnCW-Z7qv",
        "outputId": "4d1a280e-b896-45da-d44e-3903ac4f3cf9"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt \\\n",
        "    -O /tmp/sonnets.txt\n",
        "data = open('/tmp/sonnets.txt').read()\n",
        "\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# create input sequences using list of tokens\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "\n",
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "label = ku.to_categorical(label, num_classes=total_words)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-29 19:07:50--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.197.128, 74.125.142.128, 74.125.195.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.197.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 93578 (91K) [text/plain]\n",
            "Saving to: ‘/tmp/sonnets.txt’\n",
            "\n",
            "\r/tmp/sonnets.txt      0%[                    ]       0  --.-KB/s               \r/tmp/sonnets.txt    100%[===================>]  91.38K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2021-05-29 19:07:50 (127 MB/s) - ‘/tmp/sonnets.txt’ saved [93578/93578]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9vH8Y59ajYL",
        "outputId": "e0a77b99-e2f6-4123-d492-d16fbbee7f2c"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(150, return_sequences = True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 10, 100)           321100    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 10, 300)           301200    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 10, 300)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               160400    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1605)              162105    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3211)              5156866   \n",
            "=================================================================\n",
            "Total params: 6,101,671\n",
            "Trainable params: 6,101,671\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIg2f1HBxqof",
        "outputId": "e2cb2957-1cb6-4d30-b0ad-124ef91b2e20"
      },
      "source": [
        " history = model.fit(predictors, label, epochs=100, verbose=1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "484/484 [==============================] - 16s 12ms/step - loss: 6.8978 - accuracy: 0.0213\n",
            "Epoch 2/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 6.4992 - accuracy: 0.0226\n",
            "Epoch 3/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 6.3974 - accuracy: 0.0235\n",
            "Epoch 4/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 6.2880 - accuracy: 0.0292\n",
            "Epoch 5/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 6.1933 - accuracy: 0.0331\n",
            "Epoch 6/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 6.1093 - accuracy: 0.0384\n",
            "Epoch 7/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 6.0342 - accuracy: 0.0402\n",
            "Epoch 8/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 5.9525 - accuracy: 0.0451\n",
            "Epoch 9/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 5.8605 - accuracy: 0.0518\n",
            "Epoch 10/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 5.7573 - accuracy: 0.0581\n",
            "Epoch 11/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 5.6542 - accuracy: 0.0614\n",
            "Epoch 12/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 5.5515 - accuracy: 0.0675\n",
            "Epoch 13/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 5.4492 - accuracy: 0.0712\n",
            "Epoch 14/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 5.3427 - accuracy: 0.0790\n",
            "Epoch 15/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 5.2368 - accuracy: 0.0864\n",
            "Epoch 16/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 5.1317 - accuracy: 0.0924\n",
            "Epoch 17/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 5.0280 - accuracy: 0.1026\n",
            "Epoch 18/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 4.9216 - accuracy: 0.1100\n",
            "Epoch 19/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 4.8163 - accuracy: 0.1187\n",
            "Epoch 20/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 4.7146 - accuracy: 0.1280\n",
            "Epoch 21/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 4.6117 - accuracy: 0.1402\n",
            "Epoch 22/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 4.4966 - accuracy: 0.1521\n",
            "Epoch 23/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 4.4005 - accuracy: 0.1636\n",
            "Epoch 24/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 4.2912 - accuracy: 0.1759\n",
            "Epoch 25/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 4.1853 - accuracy: 0.1848\n",
            "Epoch 26/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 4.0840 - accuracy: 0.2009\n",
            "Epoch 27/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 3.9830 - accuracy: 0.2157\n",
            "Epoch 28/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 3.8736 - accuracy: 0.2319\n",
            "Epoch 29/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 3.7782 - accuracy: 0.2496\n",
            "Epoch 30/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 3.6842 - accuracy: 0.2650\n",
            "Epoch 31/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 3.6025 - accuracy: 0.2800\n",
            "Epoch 32/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 3.4959 - accuracy: 0.3025\n",
            "Epoch 33/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 3.4017 - accuracy: 0.3231\n",
            "Epoch 34/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 3.3144 - accuracy: 0.3411\n",
            "Epoch 35/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 3.2349 - accuracy: 0.3608\n",
            "Epoch 36/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 3.1550 - accuracy: 0.3780\n",
            "Epoch 37/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 3.0685 - accuracy: 0.3971\n",
            "Epoch 38/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.9959 - accuracy: 0.4126\n",
            "Epoch 39/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.9255 - accuracy: 0.4289\n",
            "Epoch 40/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.8541 - accuracy: 0.4439\n",
            "Epoch 41/100\n",
            "484/484 [==============================] - 6s 11ms/step - loss: 2.7944 - accuracy: 0.4601\n",
            "Epoch 42/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.7242 - accuracy: 0.4740\n",
            "Epoch 43/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.6563 - accuracy: 0.4886\n",
            "Epoch 44/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.5876 - accuracy: 0.5029\n",
            "Epoch 45/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.5434 - accuracy: 0.5121\n",
            "Epoch 46/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.4896 - accuracy: 0.5272\n",
            "Epoch 47/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.4288 - accuracy: 0.5378\n",
            "Epoch 48/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.3869 - accuracy: 0.5481\n",
            "Epoch 49/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.3201 - accuracy: 0.5687\n",
            "Epoch 50/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.2775 - accuracy: 0.5766\n",
            "Epoch 51/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.2327 - accuracy: 0.5849\n",
            "Epoch 52/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.1831 - accuracy: 0.5962\n",
            "Epoch 53/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.1456 - accuracy: 0.6054\n",
            "Epoch 54/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.1105 - accuracy: 0.6138\n",
            "Epoch 55/100\n",
            "484/484 [==============================] - 6s 11ms/step - loss: 2.0613 - accuracy: 0.6235\n",
            "Epoch 56/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 2.0330 - accuracy: 0.6314\n",
            "Epoch 57/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.9988 - accuracy: 0.6341\n",
            "Epoch 58/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.9460 - accuracy: 0.6464\n",
            "Epoch 59/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.9107 - accuracy: 0.6576\n",
            "Epoch 60/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.8785 - accuracy: 0.6665\n",
            "Epoch 61/100\n",
            "484/484 [==============================] - 6s 11ms/step - loss: 1.8497 - accuracy: 0.6718\n",
            "Epoch 62/100\n",
            "484/484 [==============================] - 6s 11ms/step - loss: 1.8219 - accuracy: 0.6740\n",
            "Epoch 63/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.7819 - accuracy: 0.6832\n",
            "Epoch 64/100\n",
            "484/484 [==============================] - 6s 11ms/step - loss: 1.7474 - accuracy: 0.6922\n",
            "Epoch 65/100\n",
            "484/484 [==============================] - 6s 11ms/step - loss: 1.7284 - accuracy: 0.6956\n",
            "Epoch 66/100\n",
            "484/484 [==============================] - 6s 11ms/step - loss: 1.6972 - accuracy: 0.7020\n",
            "Epoch 67/100\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 1.6843 - accuracy: 0.7048\n",
            "Epoch 68/100\n",
            "484/484 [==============================] - 6s 11ms/step - loss: 1.6520 - accuracy: 0.7110\n",
            "Epoch 69/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.6090 - accuracy: 0.7196\n",
            "Epoch 70/100\n",
            "484/484 [==============================] - 6s 11ms/step - loss: 1.5922 - accuracy: 0.7222\n",
            "Epoch 71/100\n",
            "484/484 [==============================] - 6s 12ms/step - loss: 1.5824 - accuracy: 0.7246\n",
            "Epoch 72/100\n",
            "484/484 [==============================] - 6s 11ms/step - loss: 1.5608 - accuracy: 0.7288\n",
            "Epoch 73/100\n",
            "484/484 [==============================] - 6s 11ms/step - loss: 1.5242 - accuracy: 0.7383\n",
            "Epoch 74/100\n",
            "484/484 [==============================] - 6s 11ms/step - loss: 1.5094 - accuracy: 0.7406\n",
            "Epoch 75/100\n",
            "484/484 [==============================] - 6s 11ms/step - loss: 1.4867 - accuracy: 0.7460\n",
            "Epoch 76/100\n",
            "484/484 [==============================] - 6s 11ms/step - loss: 1.4708 - accuracy: 0.7458\n",
            "Epoch 77/100\n",
            "484/484 [==============================] - 6s 11ms/step - loss: 1.4458 - accuracy: 0.7482\n",
            "Epoch 78/100\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 1.4422 - accuracy: 0.7491\n",
            "Epoch 79/100\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 1.4178 - accuracy: 0.7543\n",
            "Epoch 80/100\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 1.4018 - accuracy: 0.7608\n",
            "Epoch 81/100\n",
            "484/484 [==============================] - 6s 11ms/step - loss: 1.3784 - accuracy: 0.7634\n",
            "Epoch 82/100\n",
            "484/484 [==============================] - 6s 11ms/step - loss: 1.3490 - accuracy: 0.7700\n",
            "Epoch 83/100\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 1.3347 - accuracy: 0.7702\n",
            "Epoch 84/100\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 1.3309 - accuracy: 0.7726\n",
            "Epoch 85/100\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 1.3193 - accuracy: 0.7729\n",
            "Epoch 86/100\n",
            "484/484 [==============================] - 6s 11ms/step - loss: 1.3033 - accuracy: 0.7751\n",
            "Epoch 87/100\n",
            "484/484 [==============================] - 6s 11ms/step - loss: 1.2921 - accuracy: 0.7767\n",
            "Epoch 88/100\n",
            "484/484 [==============================] - 6s 11ms/step - loss: 1.2731 - accuracy: 0.7833\n",
            "Epoch 89/100\n",
            "484/484 [==============================] - 6s 11ms/step - loss: 1.2587 - accuracy: 0.7820\n",
            "Epoch 90/100\n",
            "484/484 [==============================] - 6s 11ms/step - loss: 1.2510 - accuracy: 0.7863\n",
            "Epoch 91/100\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 1.2467 - accuracy: 0.7859\n",
            "Epoch 92/100\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 1.2382 - accuracy: 0.7868\n",
            "Epoch 93/100\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 1.2317 - accuracy: 0.7867\n",
            "Epoch 94/100\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 1.2086 - accuracy: 0.7903\n",
            "Epoch 95/100\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 1.1849 - accuracy: 0.7961\n",
            "Epoch 96/100\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 1.1888 - accuracy: 0.7958\n",
            "Epoch 97/100\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 1.1733 - accuracy: 0.7971\n",
            "Epoch 98/100\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 1.1658 - accuracy: 0.7975\n",
            "Epoch 99/100\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 1.1582 - accuracy: 0.7967\n",
            "Epoch 100/100\n",
            "484/484 [==============================] - 5s 11ms/step - loss: 1.1427 - accuracy: 0.8034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "1fXTEO3GJ282",
        "outputId": "a4cb28d8-6597-4bd9-b588-885922b4b619"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "loss = history.history['loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.title('Training accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "plt.title('Training loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-31858f16e7b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vc6PHgxa6Hm"
      },
      "source": [
        "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}